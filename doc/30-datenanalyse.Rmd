\cleardoublepage
# Datenanalyse {#datenanalyse}
Dieses Kapitel liefert einen Überblick über die Methodik sowie die Ergebnisse der Datenanalyse.
Zunächst wird der verwendete Datensatz präsentiert und Kritik daran erörtert.
Weiterhin wird dargelegt, wie die betrachteten Topic-Modelle erzeugt werden und welche Methoden der sozialen Netzwerkanalyse Anwendung finden, sowie welche Software-Komponenten jeweils zum Einsatz kommen.
Im zweiten Teil des Kapitels werden dann die Ergebnisse vorgestellt, ohne dabei jedoch einer Interpretation zu weit vorzugreifen.

## Methodik {#methodik}

### Datensatz

Die Grundlage der Analyse bildet ein frei zugänglicher Datensatz mit Reddit-Kommentaren.
Jason Baumgartner, der unter dem Pseudonym *stuck_in_the_matrix*^[https://www.reddit.com/user/stuck_in_the_matrix] selbst auf Reddit aktiv ist, stellt monatliche Zusammenfassungen aller erstellten Kommentare zum Download bereit [@Baumgartner].
Diese reichen zum gegenwärtigen Zeitpunkt von Oktober 2018 zurück bis Dezember 2005.

#### Struktur
Die monatlichen Datensätze liegen in Form von Textdateien vor, in denen jede Zeile einen Kommentar sowie Metadaten enthält.
Das maschinenlesbare JSON-Format, in dem die Daten abgelegt sind, ermöglicht dabei eine effiziente computergestützte Auswertung.
Tabelle \@ref(tab:importantkeys) führt die für diese Arbeit relevanten Schlüssel-Wert-Paare des Datensatzes auf.
Der Schlüssel *parent_id* bezeichnet dabei das Element, auf welches sich der Kommentar bezieht.
Dies können Beiträge oberster Ordnung sein, sog. "Links", oder selbst Kommentare [@Reddit2018].
Zu beachten ist hier insbesondere, dass der eigentliche Textinhalt des Kommentars für diese Auswertung nicht genutzt wird.

```{r importantkeys}
knitr::kable(
  tribble(
    ~Schlüssel, ~Wert,
    "author", "Nutzername des Kommentar-Erstellers",
    "id", "eindeutige ID des Kommentars",
    "parent_id", "eindeutige ID des Elements, auf das sich der Kommentar bezieht",
    "subreddit", "Name des Subreddits, in dem der Kommentar erstellt wurde"
  ),
  booktabs = TRUE,
  caption = "wichtige Schlüssel-Wert-Paare des Datensatzes"
)
```

#### Kohärenz des Datensatzes
Im März 2018 haben Gaffney und Matias eine Analyse des Baumgartner-Korpus vorgelegt [@Gaffney2018].
Der vollständige Korpus enthält neben Kommentaren auch Datensätze mit allen monatlich erstellten Beiträgen, im folgenden auch "Submissions" genannt.
Gaffney und Matias kommen zu dem Schluss, dass die Erfassung sowohl der Submissions als auch der Kommentare Lücken aufweist, also Elemente gänzlich nicht im Datensatz vorhanden sind.
Für den Gegenstand der vorliegenden Arbeit ist dieser Umstand insofern von Bedeutung, als dass fehlende Kommentare die Topic-Affinität von Nutzern verzerren können, etwa wenn ein Topic, in dem der Nutzer durchaus aktiv war, aufgrund fehlender Daten unterrepräsentiert ist.
Auch Gaffney und Matias stellen fest, dass Studien, welche auf die vollständige Historie von Nutzern zugreifen, dem höchsten Risiko ausgesetzt sind, lückenhafte Daten zu betrachten [@Gaffney2018].

Reddit weist jedem Kommentar eine eindeutige numerische ID zu.
Das von Baumgartner eingesetzte System nimmt zusammenhängende Blöcke von jeweils 100 solcher IDs und versucht, die zugehörigen Kommentare über die Reddit-API^[https://api.reddit.com/] aufzulösen [@Baumgartner2018a]. 
Da Reddit auch Anfragen nach gelöschten Elementen sinnvoll beantwortet, also nicht etwa mit einer Fehlermeldung, sollte ein Bereich von 100 sequentiellen IDs auch vollständig im Datensatz abgebildet sein, inklusive als gelöscht markierter Elemente.
Gaffney und Matias stellen jedoch für den Zeitraum Dezember 2005 bis Februar 2016 fest, dass 943.755 Kommentar- und 1.539.583 Beitrags-IDs nicht in den Datensätzen enthalten sind.
Als mögliche Gründe für das Fehlen nennen Gaffney und Matias dreierlei: sog. "dangling references", also Verweise, bei denen das Element, auf das verwiesen wird, nicht auffindbar ist; öffentlich zugängliche Daten, die aus unbekanntem Grund nicht von Reddit an Baumgartners System übertragen wurden; oder Daten aus als privat eingestuften Communities, die nicht öffentlich sondern nur von Mitgliedern mit Zugangsberechtigung einsehbar sind [@Gaffney2018].

(ref:gf4cap) Anteil fehlender Kommentare. Die hellblauen Quadrate (obere Linie) stellen den gleitenden Mittelwert fehlender Kommentare in Prozent dar, die mittelblauen Punkte (mittlere Linie) den prozentualen Anteil fehlender Kommentare, und die dunkelblauen Kreuze (untere Linie)  die kumulierte Gesamtzahl fehlender Kommentare [@Gaffney2018]
```{r gf4, fig.cap='(ref:gf4cap)'}
knitr::include_graphics("./images/gaffneymatias_fig4.eps")
```
In Abbildung \@ref(fig:gf4) stellen die mittelblauen Punkte den Anteil fehlender Kommentare in Prozent dar.
Ab etwa April 2006 beginnt dieser Anteil zu sinken, fällt ab etwa August 2007 stark ab und stabilisiert sich ab etwa November 2007 im niedrigen einstelligen Bereich.
Um den Einfluss fehlender Kommentare so gering wie möglich zu halten, wurden daher für die vorliegende Arbeit die Datensätze beginnend mit November 2007 bis einschließlich Februar 2018 ausgewertet.  
Jason Baumgartner hat als Folge der Veröffentlichung von Gaffney und Matias angekündigt, fehlende Kommentare und Beiträge nachträglich zu erfassen [@Baumgartner2018].

### Stichprobe von Nutzern {#stichprobe}

Nachdem der Datensatz einer zeitlichen Einschränkung unterworfen wurde, müssen Kriterien für die Auswahl von Nutzern herangezogen werden.
Dieser Abschnitt gibt Aufschluss über die Altersverteilung von Nutzern im Datensatz und beschreibt die Ziehung einer Stichprobe für die Datenanalyse.

#### Altersverteilung

```{r db-months-active}
# only read the entire table if no result exists yet
if (!(exists("res") && is.data.frame(res) && nrow(res) > 0)) {
  con <-
    dbConnect(RSQLite::SQLite(),
              "/run/media/oliver/Elements SE/reddit.db")
  
  res <- dbGetQuery(con, glue_sql("SELECT * FROM `months_active` WHERE `author` != '[deleted]'", .con = con))
  
  dbDisconnect(con)
}

res_counted <- res %>% 
  count(active)

num_users <- nrow(res)
summary_age <-
  res %>% 
  summarise(
    N = n(),
    "arithm. Mittel" = mean(active),
    SD = sd(active),
    Min = min(active),
    Q1 = quantile(active, .25),
    Median = median(active),
    Q3 = quantile(active, .75),
    Max = max(active)
  )
```
Nach der zeitlichen Eingrenzung der Daten liegen für den betrachteten Zeitraum von 124 Monaten etwa 3,6 Milliarden `r # fmt(3615667286)` Kommentare vor, verfasst von ca. 28 Millionen `r # fmt(num_users)` Nutzern.
Für jeden dieser Nutzer wurde bestimmt, in wie vielen Monaten er insgesamt im Datensatz enthalten ist; nachfolgend wird dies als "virtuelles Alter" oder schlicht "Alter" des Nutzers bezeichnet.

(ref:summary-age-lab) Kennzahlen der Altersverteilung.
```{r summary-age-tab}
knitr::kable(
  summary_age,
  digits = 2,
  align = c("l", "c", "c", "c", "c", "c", "c", "c"),
  booktabs = TRUE,
  format.args = list(big.mark = ".", decimal.mark = ","),
  caption = "(ref:summary-age-lab)"
)
```

Tabelle \@ref(tab:summary-age-tab) bietet eine Übersicht über die Verteilung der Alterswerte.
50% der Nutzer sind zwischen einem und sechs Monaten auf Reddit aktiv (unteres resp. oberes Quartil), und 50% sind in zwei Monaten oder weniger enthalten (Median).
Der arithmetische Altersdurchschnitt liegt bei etwa sieben Monaten.
Da ein Nutzer mindestens einen Kommentar verfasst haben muss, um gezählt zu werden, liegt das minimale Alter bei einem Monat.

```{r age-distribution-1, fig.cap="Verteilung der Alterswerte."}
ggplot(data = res_counted) +
  geom_bar(
    mapping = aes(x = active, y = n / sum(n)),
    stat = "identity"
  ) +
  labs(x = "Alter in Monaten",
       y = "Anteil Nutzer") +
  scale_x_continuous(breaks = seq(0, 124, by = 12)) +
  scale_y_continuous(labels = scales::percent_format(decimal.mark = ",", accuracy = 1))
```

Das Histogramm in Abbildung \@ref(fig:age-distribution-1) veranschaulicht das hohe Vorkommen eher kleiner Werte.
Die ALtersverteilung weist einen sog. "Long Tail" auf: sehr viele Nutzer sind eher kurz aktiv, während Nutzer mit eher langer Aktivität nur einen gerinen Anteil ausmachen.
Die lineare Skala des Histograms macht es schwierig, die Werte des Long Tail sinnvoll darzustellen.
Abbildung \@ref(fig:age-distribution-2) nutzt daher eine logarithmische Darstellung.

```{r age-distribution-2, fig.cap="Verteilung der Alterswerte, logarithmische Darstellung."}
ggplot(data = res_counted) +
  geom_bar(
    mapping = aes(x = active, y = n),
    stat = "identity",
  ) +
  labs(x = "Alter in Monaten",
       y = "Anzahl Nutzer") +
  scale_x_continuous(breaks = seq(0, 124, by = 12)) +
  scale_y_log10(label = scales::comma_format(decimal.mark = ",", big.mark = "."))
```
Markant ist bei dieser Visualisierung der Ausschlag am äußersten rechten Rand.
Offenbar entfallen auf die Altersgruppe "124 Monate" noch einmal deutlich mehr Nutzer als noch auf die vorhergehenden Gruppen.
In der Tat beträgt der Unterschied zwischen den beiden letzten Altersgruppen `r subset(res_counted, active == 124, n) - subset(res_counted, active == 123, n)` Nutzer
Obwohl an dieser Stelle keine Erklärung für diese Beobachtung geliefert werden kann, ist es denkbar, dass es einen "harten Kern" von Nutzern gibt, die Reddit seit langer Zeit, möglicherweise sogar von Anfang an nutzen, und regelmäßig aktiv sind.

#### Kriterien für die Stichprobe {#kriterien}

Um Nutzer für die weitere Datenanalyse auszuwählen, wurden zwei Kriterien festgelegt.
Zum einen sollte ein Nutzer über einen ausreichend langen Zeitraum hinweg aktiv sein.
Hierdurch wird sichergestellt, dass zeitliche Verläufe möglichst keine Lücken enthalten und genügend Daten vorliegen, um Trends zu identifizieren.
Zum anderen sollte der Nutzer ein Mindestmaß an Interaktionen pro Monat aufweisen, damit Aussagen sowohl über seine thematische Affinität als auch die sozialen Kreise möglich sind, in denen er sich bewegt.

Um dem zeitlichen Kriterium zu genügen wird eine zufällige Auswahl aus den ältesten `r fmt(10000)` Nutzern getroffen.
Indem nur Nutzer einbezogen werden, die über die gesamten 124 Monate hinweg mindestens 50 Kommentare pro Monat erstellt haben, wird der Datensatz weiter gefiltert und dem Volumen-Kriterium entsprochen.
Das Histogramm in Abbildung \@ref(fig:sample-users-hist) zeigt die zugehörige Verteilung der Altersgruppen nachdem die beiden Einschränkungen vorgenommen wurden.

```{r sample-users}
thresh_size <- 10000
thresh_posts <- 50

# cut off the `thresh_size` oldest users
top_age <- res[order(res$active, decreasing = TRUE)[1:thresh_size], ]

min_age_in_sample <- min(top_age$active)
date_range <-
  seq.Date(
    from = as.Date("2007-11-01"),
    to = as.Date("2018-02-01"),
    by = "1 month"
  )

if (!(exists("total_posts") && nrow(total_posts) > 0)) {
  con <-
    dbConnect(
      RSQLite::SQLite(),
      "/run/media/oliver/Elements SE/posts_per_month_and_totals.db"
    )
  
  total_posts <-
    dbGetQuery(
      con,
      glue_sql(
        "SELECT * FROM posts ORDER BY months_active DESC LIMIT {thresh_size}",
        .con = con
      )
    )
  
  dbDisconnect(con)
}

top_active_users <-
  left_join(top_age, total_posts, by = c("author" = "username")) %>%
  filter(total_count / active >= thresh_posts) %>%
  count(active)
```

```{r sample-users-hist, fig.cap="Altersverteilung nach Einschränkungen."}
top_active_users %>%
  ggplot() +
  geom_bar(
    mapping = aes(x = active, y = n),
    stat = "identity"
  ) +
  labs(
    x = "Alter in Monaten",
    y = "Anzahl Nutzer"
  )
```

Wegen der Auswahl der `r fmt(thresh_size)` ältesten Nutzer ist die erste, "jüngste" Säule nicht vollständig gefüllt.
Das Mindestalter in dieser neuen Verteilung liegt bei `r min_age_in_sample` Monaten.
Dies entpricht einer Überschneidung zu `r round(min_age_in_sample / length(date_range) * 100, 2)`% mit dem gesamten Untersuchungszeitraum von 124 Monaten.

### Topic-Modelle

Für alle Nutzer, die über dem Durchschnittsalter von `r round(subset(summary_age, TRUE, "arithm. Mittel"), 2)` Monaten liegen, wurde festgehalten, in welchen Subreddits sie kommentiert hatten.
Durch diese Altersbeschränkung wird verhindert, dass Subreddits in das Topic-Modell einbezogen werden, die ausschließlich von Nutzern aufgesucht werden, die die Plattform nach kurzer Aktivität verlassen.

```{r sub-topic-mapping}
library("R.utils")
library("tidyverse")
nsubs <- countLines("../data/subreddits/subreddits_unique.txt")

subreddits <- readLines("../data/lda/the_subreddits.dat")
# first line contains total count of subreddits
subreddits <- subreddits[-1]

topics <- readLines("../data/lda/model-final.theta") %>% 
  str_split(" ") %>% 
  map_int(which.max)

subreddit_topic_mapping <- tibble(sub = subreddits, topic = factor(topics))

```

Für die so erhaltenen `r fmt(nsubs)` Subreddits wurden über die Reddit-API jeweils maximal 50 Beiträge aus dem Listing "Top" abgerufen.
Diese Sortierung liefert Beiträge mit dem besten Score, also der Differenz aus Up- und Downvotes [@RedditSrc].
In der Folge erhält man so diejenigen Beiträge, die von der Community am besten bewertet wurden.
In dieser Arbeit wird davon ausgegangen, dass ein Beitrag mit hohem Score auch repräsentativ für die Inhalte der Community ist.

Beim Abrufen der Top-Listings über die Reddit-API traten zum Teil zwei Arten von HTTP-Fehlern auf: "403 Forbidden" sowie "404 Not found".
Da keine weiteren Informationen zu den Fehlern übermittelt wurden, können nur Vermutungen über deren Ursache angestellt werden.
Da Listings auf Subreddit- und nicht auf Beitragsebene abgerufen werden, ist ein Fehler immer im Kontext des Zugriffs auf eine Community zu verstehen.
In diesem Fall könnte der Grund für einen 403-Fehler auf mangelnde Zugriffsrechte auf das Subreddit zurückzuführen sein, sprich: nur Mitglieder dürfen Beiträge lesen und schreiben, die Community ist privat.
Ähnlich ist ein 404-Fehler zu interpretieren: obwohl das Subreddit im Datensatz enthalten ist, kann es zum gegenwärtigen Zeitpunkt nicht abgerufen werden; aller Voraussicht nach wurde es gesperrt oder gelöscht.

Die Titel der `r fmt(length(subreddits))` erfolgreich abgerufenen Beiträge wurden konkateniert und zusammen mit dem Namen des Subreddits gespeichert.
Jedes Subreddit entspricht damit einem Dokument, dessen Inhalt die Titel der Top 50 Beiträge bilden.
Die Inhalte wurden in Kleinschreibung umgewandelt und um Satzzeichen und Ziffern bereinigt; redundante Leerzeichen wurden entfernt.
Zudem wurden Dokumente verworfen, deren Inhalt weniger als 500 Zeichen umfasste.
Die so erhaltene Sammlung an Dokumenten diente dem LDA-Algorithmus als Eingabe.

Wegen der hohen Anzahl an Dokumenten wurde wurde eine effiziente Implementierung des LDA-Algorithmus benötigt.
Die Wahl fiel dabei auf "GLDA", eine "verbesserte Version" [@Lu2013] der "GibbsLDA++"-Implementierung [@Phan2007], welche sich die hohe Rechenleistung moderner Grafikkarten zunutze macht.
Die Start-Parameter des Algorithmus führt Tabelle \@ref(tab:lda-params) auf.

```{r lda-params}
knitr::kable(
  tribble(~ Parameter, ~ Wert,
          #-----------------#
          "$\\alpha$", round(0.195312, 3),
          "$\\beta$", 0.100000,
          "k", "256",
          "niter", "2000"),
  escape = FALSE,
  align = c("c"),
  booktabs = TRUE,
  format.args = list(big.mark = ".", decimal.mark = ","),
  caption = "Startparameter des LDA-Algorithmus"
)
```

Da sich eine Evaluation verschiedener *k*-Parameter wegen der hohen Zahl an Dokumenten als schwierig herausstellte, wurde die Zahl der zu bestimmenden Topics auf 256 festgelegt.
Unklar ist, ob diese Zahl in der Nähe eines Optimums liegt; dies festzustellen wird jedoch nicht Gegenstand dieser Arbeit sein.

Der LDA-Algorithmus beruht auf der Annahme, dass sich jedes Dokument aus verschiedenen (latenten) Themen^[In dieser Arbeit wird  hauptsächlich die englische Wortform "topic(s)" gebraucht, um explizit die algorithmisch bestimmten Themenkomplexe zu bezeichnen] zusammen setzt.
Das Ergebnis liefert für jedes Dokument eine Wahrscheinlichlichkeitsverteilung über die zu bestimmenden Topics.
Für die weitere Analyse in dieser Arbeit wird aus dieser Verteilung von Topic-Wahrscheinlichkeiten eine 1:1-Zuordnung abgeleitet, indem für jedes Dokument das Topic als maßgebend angesehen wird, dem der Algorithmus die höchste Wahrscheinlichkeit zuweist.

```{r topic-assignments}
sub_topic_hist <-
  subreddit_topic_mapping %>% 
  count(topic) %>% 
  mutate(rel = n/sum(n))

sub_topic_summary <-
  sub_topic_hist %>% 
  summarise(
    N = n(),
    "arithm. Mittel" = mean(n),
    SD = sd(n),
    Min = min(n),
    Q1 = quantile(n, .25),
    Median = median(n),
    Q3 = quantile(n, .75),
    Max = max(n)
  )
```

(ref:topic-assignments-total-lab) Anzahl der Zuordnungen von Subreddits zu Topics. Die Gesamtzahl aller kategorisierten Topics beträgt `r fmt(length(subreddits))`, die Zahl der Topics 256. Aus Gründen der besseren Darstellung wurde auf eine Auszeichnung der x-Achse verzichtet; die Sortierung erfolgt absteigend nach Anzahl der Subreddits.
```{r topic-assignments-total, fig.cap="(ref:topic-assignments-total-lab)"}
sub_topic_hist %>% 
ggplot() +
  geom_col(
    aes(reorder(topic, -n), n)
    
  ) +
  labs(
    x = "Topic",
    y = "Anzahl Subreddits"
  ) +
  scale_y_log10(labels = scales::comma_format(big.mark = ".", decimal.mark = ",")) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```


Abbildung \@ref(fig:topic-assignments-total) stellt die Zuordnung von Subreddits zu Topics als Histogramm dar.
Jede Klasse entlang der x-Achse entspricht einem der 256 Topics, die der LDA-Algorithmus identifizieren sollte.
Die y-Achse ist logarithmisch skaliert und notiert die Anzahl an Subreddits, die dem jeweiligen Topic zugewiesen wurden.
Wie bereits zuvor beim Alter der Nutzer zeigt diese Darstellung eine Verteilung mit einem Long Tail: auf einen großen Teil der Topics entfallen vergleichsweise wenige Subreddits; 50% der Topics sind weniger als neun Subreddits zugeordnet (siehe auch Tabelle \@ref(tab:topic-assignments-summary)).

(ref:topic-assignments-summary-lab) Kennzahlen der Topic-Verteilung
```{r topic-assignments-summary}
knitr::kable(
  sub_topic_summary,
  digits = 2,
  align = c("l", "c", "c", "c", "c", "c", "c", "c"),
  booktabs = TRUE,
  format.args = list(big.mark = ".", decimal.mark = ","),
  caption = "(ref:topic-assignments-summary-lab)"
)
```


(ref:topic-assignments-top-lab) Anzahl der Zuordnungen von Subreddits zu Topics. Dargestellt sind alle Topics, denen 500 oder mehr Subreddits zugeordnet wurden. Die Sortierung erfolgt analog zu Abbildung \@ref(fig:topic-assignments-total) nach Anzahl Zuordnungen in absteigender Folge.
```{r topic-assignments-top, fig.cap="(ref:topic-assignments-top-lab)", fig.height=5.5}
large_topics <- subset(sub_topic_hist, n >= 500) 

large_topics %>% 
ggplot() +
  geom_point(
    mapping = aes(reorder(topic, n), n)
  ) +
  labs(
    x = "Topic",
    y = "Anzahl Subreddits"
  ) +
  scale_y_continuous(labels = scales::comma_format(big.mark = ".", decimal.mark = ","), limits = c(500, 50000), breaks = c(500, 10000, 20000, 30000, 40000, 50000)) +
  coord_flip()
```

```{r top-words}
# this chunk is used merely to generate the top 25 words in the largest topics.
# this data will be used later on in the appendix to generate a table containing the topics and words.
large_topics <- large_topics[order(large_topics$n, decreasing = TRUE), ]

top_words <- read.csv("../data/lda/top_words.csv")
top_words <- top_words[as.integer(large_topics$topic), ]
top_words <- top_words[, 1:25]

top_words_pretty <- tibble(
  Topic = large_topics$topic,
  n = large_topics$n,
  Wörter = apply(top_words, 1, paste, collapse = ", ")
)

write.csv(x = top_words_pretty, file = "../data/lda/largest_topics_top_words.csv", row.names = FALSE)

```

LDA liefert nicht nur für jedes Dokument eine Verteilung von Topics, sondern auch zu jedem Topic eine Wahrscheinlichkeitsverteilung von Wörtern.
Sortiert man die Auftretenswahrscheinlichkeiten der Wörter eines Topics, erhält man die für dieses Thema charakteristischen Begriffe.
Tabelle \@ref(tab:app-top-words-tab) im Anhang enthält für alle Topics, denen mindestens 500 Subreddits zugeordnet wurden, die 25 häufigsten Wörter.
Dabei fällt auf, dass die Topics 235, 122, 69 und 194 beinahe ausschließlich aus Stoppwörtern bestehen.
Da dies keine sinnvollen Rückschlüsse auf Nutzerinteressen zulassen, werden bei sie der weiteren Analyse zwar aufgeführt, jedoch nicht näher berücksichtigt.

### Interaktionsgraphen aus Kommentaren

Die Kommentarverläufe von Reddit lassen sich als Interaktionsgraphen modellieren.
Jeder Knoten in einem solchen Graph stellt einen Akteur in einem sozialen Netzwerk dar.
Zwischen Akteuren manifestieren sich Kanten, wenn sie miteinander interagieren, in diesem Fall in Form von Kommentaren auf Reddit.
Die Richtung der Kanten gibt dabei an, welcher Nutzer den Kommentar verfasst hat (Quelle) bzw. an welchen Nutzer der Kommentar gerichtet ist (Senke).
Im Datensatz sind Kanten über die Beziehung zwischen den *id*- bzw. *parent_id*-Attributen realisiert.
Seien dazu $U, V$ Akteure im sozialen Netzwerk und $K_U, K_V$ von $U$ resp. $V$ verfasste Reddit-Kommentare.
Zwischen $U$ und $V$ wird eine gerichtete Kante $(u,v)$ eingefügt, wenn gilt:
$K_{U}.parent\_id = K_{V}.id$.

Für jeden wie in Abschnitt \@ref(kriterien) beschrieben ausgewählten Nutzer werden monatliche Interaktionsgraphen bestimmt.
Da diese einen zeitlich abgegrenzten Ausschnitt aus dem gesamten sozialen Netzwerk eines Nutzers darstellen, werden sie im folgenden auch als Snapshot-Graphen oder einfach Snapshots bezeichnet.
Da ausgehend von einem Nutzer dessen unmittelbare Kontakte erfasst werden, spricht man hier zudem von egozentrischen Netzwerken.
Dabei ist zu beachten, dass abweichend von gängigen Definitionen des Begriffs (etwa [@Wasserman1994, S. 42], [@Wolf2010]) in dieser Arbeit keine Strukturen zwischen den Alteri erfasst werden, sondern nur zwischen Ego und Alteri.

## Ergebnisse {#ergebnisse}

### Verteilung von Topics

### Fallstudie