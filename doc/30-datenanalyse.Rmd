\cleardoublepage

# Datenanalyse {#datenanalyse}
Nachdem in den vorangegangen Kapitel die für diese Arbeit relevanten Grundlagen erläutert wurden und eine Einordnung in den bestehenden Forschungsstand erfolgte, widmet sich dieses Kapitel der unternommenen Datenanalyse.
Der erste Teil dieses Kapitels liefert einen Überblick über die Forschungsmethodik, im zweiten Teil werden die Ergebnisse der Fallstudie präsentiert.

## Methodik {#methodik}
Zunächst wird der verwendete Datensatz präsentiert und Kritik daran erörtert.
Anschließend wird dargelegt, wie die betrachteten Topic-Modelle erzeugt werden und welche Methoden der sozialen Netzwerkanalyse Anwendung finden.
Das Kapitel schließt mit einem Überblick über verwendete Software-Komponenten.

### Datensatz

Die Grundlage der Analyse bildet ein frei zugänglicher Datensatz mit Reddit-Kommentaren.
Jason Baumgartner, der unter dem Pseudonym *stuck_in_the_matrix*^[https://www.reddit.com/user/stuck_in_the_matrix] selbst auf Reddit aktiv ist, unterhält monatliche Schnappschuss-Datensätze mit allen Beiträgen und Nutzerkommentaren.
Er verwendet die Reddit-API, um diese Inhalte systematisch zu archivieren und bietet sie zum Download an [@Baumgartner].
Der Zeitraum der Daten umfasst derzeit Dezember 2005 bis Oktober 2018.

```{r importantkeys}
knitr::kable(
  tribble(
    ~Feld, ~Wert,
    "author", "Nutzername des Kommentar-Autors",
    "id", "eindeutige ID des Kommentars",
    "parent_id", "eindeutige ID des Elements, auf das sich der Kommentar bezieht",
    "subreddit", "Name des Subreddits, in dem der Kommentar erstellt wurde"
  ),
  booktabs = TRUE,
  caption = "Verwendete Metadaten zu Kommentaren"
) %>% 
 kableExtra::kable_styling(latex_options = "hold_position")
```

#### Struktur
Die monatlichen Schnappschüsse liegen in Form strukturierter Textdateien vor.^[Die nachfolgenden Ausführungen beziehen sich ausschließlich auf den Kommentar-Datensatz, Bezüge zu Beiträgen werden explizit hervorgehoben.]
Diese enthalten neben dem Inhalt des Kommentars auch Metadaten wie Verfasser oder Datum.
Die für diese Arbeit relevanten Metadatenfelder sind in Tabelle \@ref(tab:importantkeys) aufgeführt.
Zu beachten ist hier insbesondere, dass der eigentliche Textinhalt des Kommentars für diese Auswertung nicht genutzt wird.
Das maschinenlesbare JSON-Format, in dem die Daten abgelegt sind, ermöglicht die effiziente computergestützte Auswertung.


#### Kohärenz
Im März 2018 haben Gaffney und Matias [@Gaffney2018] eine Analyse des gesamten Baumgartner-Korpus vorgelegt.
Sie kommen zu dem Schluss, dass die Erfassung sowohl der Beiträge als auch der Kommentare Lücken aufweist, also Elemente gänzlich nicht im Datensatz vorhanden sind.
Für diese Arbeit ist dies insofern problematisch, da fehlende Kommentare die Themenhistorie von Nutzern verzerren können.
Auch Gaffney und Matias stellen fest, dass Studien, welche auf die vollständigen Verläufe von Nutzern zugreifen, dem höchsten Risiko ausgesetzt sind, lückenhafte Daten zu betrachten [@Gaffney2018].

Das von Baumgartner eingesetzte System nutzt aus, dass jedem Kommentar auf Reddit eine eindeutige und sequentiell ansteigende numerische ID zugewiesen ist.
Geht man davon aus, dass diese ID-Sequenz bei null beginnt und bei jedem neuen Kommentar um eins erhöht wird, sollte einer Folge von $n$ Zahlen eine Menge von $n$ Kommentaren zugeordnet sein.
Auf diese Weise arbeitet auch Baumgartners Software, die zusammenhängende Blöcke von jeweils 100 solcher IDs wählt und versucht, die zugehörigen Kommentare über die Reddit-API^[https://api.reddit.com/] aufzulösen [@Baumgartner2018a]. 
Da die API auch Anfragen nach gelöschten Elementen beantwortet und hier lediglich die Felder Autor und Inhalt mit dem speziellen Wert "[deleted]" versieht, sollte ein Bereich von 100 sequentiellen IDs auch vollständig im Datensatz abgebildet sein.
Gaffney und Matias stellen jedoch für den Zeitraum Dezember 2005 bis Februar 2016 fest, dass 943.755 Kommentar- und 1.539.583 Beitrags-IDs nicht in den Datensätzen enthalten sind.
Sie nennen dafür dreierlei mögliche Gründe: sog. "dangling references", also Verweise, bei denen das Element, auf das verwiesen wird, nicht auffindbar ist; öffentlich zugängliche Daten, die aus unbekanntem Grund nicht von Reddit an Baumgartners System übertragen wurden; oder Daten aus als privat eingestuften Communities, die nicht öffentlich sondern nur von Mitgliedern mit Zugangsberechtigung einsehbar sind [@Gaffney2018].

(ref:gf4cap) Anteil fehlender Kommentare. Die hellblauen Quadrate (obere Linie) stellen den gleitenden Mittelwert fehlender Kommentare in Prozent dar, die mittelblauen Punkte (mittlere Linie) den prozentualen Anteil fehlender Kommentare, und die dunkelblauen Kreuze (untere Linie)  die kumulierte Gesamtzahl fehlender Kommentare [Abbildung aus @Gaffney2018]
```{r gf4, fig.cap='(ref:gf4cap)'}
knitr::include_graphics("./images/gaffneymatias_fig4.eps")
```

In Abbildung \@ref(fig:gf4) stellen die mittelblauen Punkte bzw. die anfangs mittlere der drei Linien den Anteil fehlender Kommentare in Prozent dar.
Ab April 2007 beginnt dieser Anteil zu sinken, fällt ab etwa August 2007 stark ab und stabilisiert sich ab November 2007 im niedrigen einstelligen Bereich.
Um den Einfluss fehlender Kommentare so gering wie möglich zu halten, wurde setzt die Auswertung der Daten im November 2007 an und erstreckt sich bis Februar 2018.  
Jason Baumgartner hat als Folge der Veröffentlichung von Gaffney und Matias angekündigt, fehlende Kommentare und Beiträge nachträglich zu erfassen [@Baumgartner2018].

### Stichprobe von Nutzern {#stichprobe}

Für die angesetzte Fallstudie wird für den Beobachtungszeitraum eine zufällige Auswahl an drei Nutzern getroffen.
Im Folgenden werden die Kriterien für die Ziehung dieser Stichprobe erläutert, die aus der Verteilung der Aktivität der Nutzer abgeleitet werden.

```{r db-months-active}
# only read the entire table if no result exists yet
if (!(exists("res") && is.data.frame(res) && nrow(res) > 0)) {
  con <-
    dbConnect(RSQLite::SQLite(),
              "/run/media/oliver/Elements SE/reddit.db")
  
  res <- dbGetQuery(con, glue_sql("SELECT * FROM `months_active` WHERE `author` != '[deleted]'", .con = con))
  
  dbDisconnect(con)
}

res_counted <- res %>% 
  count(active)

num_users <- nrow(res)
summary_age <-
  res %>% 
  summarise(
    N = n(),
    "arithm. Mittel" = mean(active),
    SD = sd(active),
    Min = min(active),
    Q1 = quantile(active, .25),
    Median = median(active),
    Q3 = quantile(active, .75),
    Max = max(active)
  )
```

Für den Beobachtungszeitraum von 124 Monaten liegen etwa 3,6 Milliarden `r # fmt(3615667286)` Kommentare vor, verfasst von ca. 28 Millionen `r # fmt(num_users)` Nutzern.
Für jeden dieser Nutzer wurde bestimmt, in wie vielen Monaten er im Datensatz enthalten ist.
Diese Aktivitätsspanne reicht von nur einem Monat bis zur Gesamtzeit von 124 Monaten, und 50% der Nutzer sind zwischen einem und sechs Monaten auf Reddit aktiv; Tabelle \@ref(tab:activity-summary-full) im Anhang bietet eine Übersicht über die Verteilung der Aktivitäten.

Stellt man diese Verteilung wie in Abbildung\ \@ref(fig:age-distribution-1) als Histogramm dar, fällt die hohe Häufung eher kurzer Aktivität auf.
Abbildung \@ref(fig:age-distribution-2) wählt für dieselbe Verteilung eine logarithmische Darstellung, die einen besseren Blick in den "Long Tail" der Verteilung bietet.
Auffällig ist bei der logarithmischen Darstellung der Ausschlag am äußersten rechten Rand.
Der Unterschied zwischen den beiden längsten Aktivitätsklassen beträgt `r subset(res_counted, active == 124, n) - subset(res_counted, active == 123, n)` Nutzer.
Obwohl an dieser Stelle keine Erklärung für diese Beobachtung geliefert werden kann, ist es möglich, dass ein "harter Kern" von Nutzern Reddit seit längerer Zeit, möglicherweise sogar von Anfang an nutzt, und regelmäßig aktiv ist.
```{r age-distribution-1, fig.cap="Verteilung der Aktivität"}
ggplot(data = res_counted) +
  geom_bar(
    mapping = aes(x = active, y = n / sum(n)),
    stat = "identity"
  ) +
  labs(x = "Aktivität in Monaten",
       y = "Anteil Nutzer") +
  scale_x_continuous(breaks = seq(0, 124, by = 12)) +
  scale_y_continuous(labels = scales::percent_format(decimal.mark = ",", accuracy = 1))
```

```{r age-distribution-2, fig.cap="Verteilung der Aktivität, logarithmische Darstellung."}
ggplot(data = res_counted) +
  geom_bar(
    mapping = aes(x = active, y = n),
    stat = "identity"
  ) +
  labs(x = "Aktivität in Monaten",
       y = "Anzahl Nutzer") +
  scale_x_continuous(breaks = seq(0, 124, by = 12)) +
  scale_y_log10(label = scales::comma_format(decimal.mark = ",", big.mark = "."))
```

```{r sample-users}
thresh_size <- 10000
thresh_posts <- 50

# cut off the `thresh_size` oldest users
top_age <- res[order(res$active, decreasing = TRUE)[1:thresh_size], ]

min_age_in_sample <- min(top_age$active)
date_range <-
  seq.Date(
    from = as.Date("2007-11-01"),
    to = as.Date("2018-02-01"),
    by = "1 month"
  )

if (!(exists("total_posts") && nrow(total_posts) > 0)) {
  con <-
    dbConnect(
      RSQLite::SQLite(),
      "/run/media/oliver/Elements SE/posts_per_month_and_totals.db"
    )
  
  total_posts <-
    dbGetQuery(
      con,
      glue_sql(
        "SELECT * FROM posts ORDER BY months_active DESC LIMIT {thresh_size}",
        .con = con
      )
    )
  
  dbDisconnect(con)
}

top_active_users <-
  left_join(top_age, total_posts, by = c("author" = "username")) %>%
  filter(total_count / active >= thresh_posts) %>%
  count(active)
```

```{r sample-users-hist, fig.cap="Verteilung der Aktivität nach vorgenommener Einschränkung auf obere 10.000 Nutzer mit mindestens 50 Kommentaren je Monat."}
top_active_users %>%
  ggplot() +
  geom_bar(
    mapping = aes(x = active, y = n),
    stat = "identity"
  ) +
  labs(
    x = "Aktivität in Monaten",
    y = "Anzahl Nutzer"
  )
```
Um sicherzustellen, dass die Historien der Nutzer möglichst frei von Lücken sind, erfolgt die Auswahl aus den `r fmt(thresh_size)` am längsten aktiven Nutzern.
Um sicherzustellen, dass für diese auch aussagekräftige Interaktionsgraphen generiert werden können, müssen sie zudem über ihren Aktivitätszeitraum hinweg pro Monat mindestens `r thresh_posts` Kommentare erstellt haben.
Abbildung \@ref(fig:sample-users-hist) zeigt die dadurch erhaltene Verteilung der Aktivitätsklassen als Histogramm.
Die kleinste Aktivitätsspanne in dieser neuen Verteilung liegt bei `r min_age_in_sample` Monaten.
Dies entspricht einer Überschneidung mit dem gesamten Untersuchungszeitraum zu ca. `r round(min_age_in_sample / length(date_range) * 100)`%; die Kennzahlen dieser Verteilung sind Tabelle\ \@ref(tab:activity-summary-cut) im Anhang zu entnehmen.

### Topic-Modelle

Subreddits stellen thematisch eigenständige Communities dar.
Da Nutzer selbst neue Subreddits erstellen können, kommt es schnell zu einer Balkanisierung verschiedener Themen.
So existiert zwar mit */r/politics* ein Subreddit, das sich politischen Inhalten verschrieben hat, aber ebenso gibt es die nuancierten Varianten */r/uspolitics* und */r/ukpolitics*.
Da nicht alle Subreddits über ihre Namen eindeutig einem Thema zugeordnet werden können, bietet sich eine computergestützte Topic-Analyse an.
Dadurch können Communities, die ähnliches Vokabular benutzen, unter einem Topic zusammengefasst werden.

#### Eingabe-Korpus

Für alle Nutzer, deren Aktivität über dem Gesamtdurchschnitt von `r round(subset(summary_age, TRUE, "arithm. Mittel"), 2)` Monaten liegt, werden die Subreddits erfasst, in denen sie Kommentare erstellen.
Durch diese Einschränkung wird verhindert, dass Subreddits in das Topic-Modell eingehen, die ausschließlich von Nutzern aufgesucht werden, die die Plattform nach kurzer Aktivität verlassen.

```{r sub-topic-mapping}
library("R.utils")
library("tidyverse")
nsubs <- countLines("../data/subreddits/subreddits_unique.txt")

subreddits <- readLines("../data/lda/the_subreddits.dat")
# first line contains total count of subreddits
subreddits <- subreddits[-1]

topics <- readLines("../data/lda/model-final.theta") %>% 
  str_split(" ") %>% 
  map_int(which.max)

subreddit_topic_mapping <- tibble(sub = subreddits, topic = factor(topics))

```

Für diese Subreddits wurden über die Reddit-API jeweils maximal 50 Beiträge aus dem Listing "Top" abgerufen.
Diese Listing liefert eine Sortierung der Beiträge mit der besten Gesamtwertung (*Score*), also der Differenz aus Up- und Downvotes [@RedditSrc].
In der Folge erhält man so diejenigen Beiträge, die von der Community am besten bewertet wurden.
In dieser Arbeit wird davon ausgegangen, dass ein Beitrag mit hohem Score auch repräsentativ für die Inhalte der Community ist.

Die Titel der abgerufenen Beiträge werden konkateniert, zu Kleinbuchstaben normalisiert, Satzzeichen und Ziffern sowie redundante Leerzeichen entfernt; Stoppwörter werden nicht entfernt und Zeichenketten mit weniger als 300 Zeichen verworfen.
Damit entspricht jedes Subreddit einem Dokument, dessen Inhalt die Titel der Top-Beiträge bilden.
Die so aufbereiteten 207.056 Dokumente dienen dem LDA-Algorithmus als Eingabe-Korpus.

#### Auswertung der Topicanalyse

Die Start-Parameter des Algorithmus sind in Tabelle\ \@ref(tab:lda-params) im Anhang aufgeführt.
Um zügig mit der eigentlichen Analyse der Nutzerhistorien und -netzwerke beginnen zu können, wurde für den Parameter *k*, der die zu bestimmende Zahl an Topics angibt, ein fester Wert von 256 gewählt.
Ob sich diese Zahl an Topics für die betrachteten Subreddits bzw. Reddit allgemein in der Nähe eines Optimums befindet, bleibt zu klären.

Der LDA-Algorithmus beruht auf der Annahme, dass sich jedes Dokument aus verschiedenen latenten Themen^[In dieser Arbeit wird  die englische Wortform "topic(s)" gebraucht, um explizit die algorithmisch bestimmten Themen zu bezeichnen.] zusammen setzt.
Das Ergebnis liefert für jedes Dokument eine Wahrscheinlichkeitsverteilung über die zu bestimmenden Topics.
Für die weitere Analyse in dieser Arbeit wird aus dieser Verteilung von Topic-Wahrscheinlichkeiten eine Zuordnung von Subreddits zu Topics abgeleitet, indem für jedes Dokument das Topic als charakteristisch angesehen wird, dem der Algorithmus die höchste Wahrscheinlichkeit zuweist.

```{r topic-assignments}
sub_topic_hist <-
  subreddit_topic_mapping %>% 
  count(topic) %>% 
  mutate(rel = n/sum(n))

sub_topic_summary <-
  sub_topic_hist %>% 
  summarise(
    N = n(),
    "arithm. Mittel" = mean(n),
    SD = sd(n),
    Min = min(n),
    Q1 = quantile(n, .25),
    Median = median(n),
    Q3 = quantile(n, .75),
    Max = max(n)
  )
```

(ref:topic-assignments-total-lab) Anzahl der Zuordnungen von Subreddits zu Topics in logarithmischer Darstellung. Jede Säule entlang der x-Achse entspricht einem der 256 Topics, die Sortierung erfolgt absteigend nach Anzahl der zugeordneten Subreddits. Aus Gründen der Darstellung wurde auf eine Auszeichnung der x-Achse verzichtet.
```{r topic-assignments-total, fig.cap="(ref:topic-assignments-total-lab)"}
sub_topic_hist %>% 
ggplot() +
  geom_col(
    aes(reorder(topic, -n), n)
  ) +
  labs(
    x = "Topic",
    y = "Anzahl Subreddits"
  ) +
  scale_y_log10(labels = scales::comma_format(big.mark = ".", decimal.mark = ",")) +
  scale_x_discrete(breaks = NULL, expand = expand_scale(mult = 0.05)) +
  theme(
    #axis.text.x = element_blank(),
    #axis.ticks.x = element_blank()
  )
```


Abbildung \@ref(fig:topic-assignments-total) stellt die Zuordnung von Subreddits zu Topics als Histogramm dar.
Auf einen großen Teil der Topics entfallen vergleichsweise wenig Subreddits; 50% der Topics sind weniger als neun Subreddits zugeordnet, 75% weniger als 97,5 (s. Tabelle\ \@ref(tab:topic-assignments-summary) im Anhang).
Offensichtlich gibt es zusätzlich zu großen bis sehr großen Topics auch eine hohe Anzahl an Nischenthemen mit weniger als 100 zugeordneten Subreddits.
\todo{Braucht man den Zoom-Plot??}

(ref:topic-assignments-top-lab) Anzahl der Zuordnungen von Subreddits zu Topics. Dargestellt sind alle Topics, denen 500 oder mehr Subreddits zugeordnet wurden. Die Sortierung erfolgt analog zu Abbildung \@ref(fig:topic-assignments-total) nach Anzahl Zuordnungen in absteigender Folge.
```{r topic-assignments-top, fig.cap="(ref:topic-assignments-top-lab)", fig.height=5.5}
large_topics <- subset(sub_topic_hist, n >= 500) 

large_topics %>%
ggplot() +
  geom_point(
    mapping = aes(reorder(topic, n), n)
  ) +
  labs(
    x = "Topic",
    y = "Anzahl Subreddits"
  ) +
  scale_y_continuous(labels = scales::comma_format(big.mark = ".", decimal.mark = ","), limits = c(500, 50000), breaks = c(500, 10000, 20000, 30000, 40000, 50000)) +
  coord_flip()
```

```{r top-words}
# this chunk is used merely to generate the top 25 words in the largest topics.
# this data will be used later on in the appendix to generate a table containing the topics and words.
large_topics <- large_topics[order(large_topics$n, decreasing = TRUE), ]

top_words <- read.csv("../data/lda/top_words.csv")
top_words <- top_words[as.integer(large_topics$topic), ]
top_words <- top_words[, 1:25]

top_words_pretty <- tibble(
  Topic = large_topics$topic,
  n = large_topics$n,
  Wörter = apply(top_words, 1, paste, collapse = ", ")
)

write.csv(x = top_words_pretty, file = "../data/lda/largest_topics_top_words.csv", row.names = FALSE)

```

LDA liefert nicht nur für jedes Dokument eine Verteilung von Topics, sondern auch zu jedem Topic eine Wahrscheinlichkeitsverteilung von Wörtern.
Sortiert man diese, erhält man die für dieses Topic charakteristischen Begriffe.
Tabelle \@ref(tab:app-top-words-tab) im Anhang enthält für alle Topics, denen mindestens 500 Subreddits zugeordnet wurden, die 25 häufigsten Wörter.
Dabei fällt auf, dass die Topics 235, 122, 69 und 194 beinahe ausschließlich aus Stoppwörtern bestehen.
Da diese insbesondere keine Schlagworte wie die anderen Topics enthalten, lässt sich nur wenig auf das tatsächliche Nutzerinteresse schließen.
Der Vollständigkeit halber werden diese Topics zwar in den Historien der Nutzer aufgeführt, jedoch keiner näheren Analyse unterzogen.

### Interaktionsgraphen aus Kommentaren

Die Kommentarverläufe von Reddit lassen sich als Interaktionsgraphen modellieren.
Jeder Knoten in einem solchen Graph stellt einen Akteur in einem sozialen Netzwerk dar.
Zwischen Akteuren manifestieren sich gerichtete Kanten, wenn sie miteinander interagieren, in diesem Fall in Form von Kommentaren auf Reddit.
Die Richtung der Kanten gibt dabei an, welcher Nutzer den Kommentar verfasst hat (Quelle) bzw. an welchen Nutzer der Kommentar gerichtet ist (Senke).
Im Datensatz sind Kanten über die Beziehung zwischen den *id*- bzw. *parent_id*-Attributen realisiert.
Seien dazu $U, V$ Akteure im sozialen Netzwerk und $K_U, K_V$ von $U$ resp. $V$ verfasste Reddit-Kommentare.
Zwischen $U$ und $V$ wird eine gerichtete Kante $(u,v)$ eingefügt, wenn gilt:
$K_{U}.parent\_id = K_{V}.id$.  
Da mehrere Interaktionen zwischen denselben Partnern möglich und erlaubt sind, handelt es sich bei den hier verwendeten Interaktionsgraphen um Multigraphen.
Da ausgehend von einem Nutzer dessen unmittelbare Kontakte erfasst werden, spricht man hier zudem von egozentrischen Netzwerken.
Dabei ist zu beachten, dass abweichend von gängigen Definitionen des Begriffs (etwa [@Wasserman1994, S. 42], [@Wolf2010]) in dieser Arbeit keine Strukturen zwischen den Alteri erfasst werden, sondern nur zwischen Ego und Alteri.

Für jeden wie in Abschnitt \@ref(kriterien) beschrieben ausgewählten Nutzer werden monatliche Interaktionsgraphen erstellt.
Da diese einen zeitlich abgegrenzten Ausschnitt aus dem gesamten sozialen Netzwerk eines Nutzers darstellen, werden sie im folgenden auch als Snapshot-Graphen bezeichnet.

### Verwendete Software {#software}

Um die Inhalte der Subreddits über die Reddit-API abzurufen, wurde die Python-Bibliothek *PRAW*\ [@PRAW] verwendet, ein API-Client speziell für Reddit.  
Die Vorbereitung der Textkorpora für die Topic-Analyse wurde mit dem Text-Mining-Package *tm*\ [@R-tm] in R realisiert.  
Für die LDA selbst wurde wegen der hohen Zahl an Inhalten eine effiziente Implementierung benötigt, die in akzeptabler Zeit ein Topic-Model berechnet. Die Wahl fiel dabei auf "GLDA"\ [@Lu2013], das sich die hohe Rechenleistung moderner Grafikkarten zunutze machen kann; die Software stellt eine Weiterentwicklung von "GibbsLDA++"\ [@Phan2007] dar.  
Die Datenanalyse erfolgt in R mit einschlägigen Bibliotheken, größtenteils aus dem *tidyverse*\ [@R-tidyverse]; Visualisierungen wurden mit *ggplot2*\ [@R-ggplot2] erstellt.  
Die Modellierung der Interaktionsgraphen wurde mit der R-Bibliothek *igraph*\ [@R-igraph] realisiert, die neben der Konstruktion auch Funktionalität zur Analyse von Netzwerken bietet.  
Die Arbeit selbst wurde mit *bookdown*\ [@R-bookdown] angefertigt, das es ermöglicht, Dokumente im Markdown-Format zu verfassen, Codeblöcke im Text zu definieren und deren Ausgabe direkt in den Text zu integrieren.

## Ergebnisse {#ergebnisse}

### Fallstudie